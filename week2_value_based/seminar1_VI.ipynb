{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov decision process\n",
    "\n",
    "This week's methods are all built to solve __M__arkov __D__ecision __P__rocesses. In the broadest sense, an MDP is defined by how it changes states and how rewards are computed.\n",
    "\n",
    "State transition is defined by $P(s' |s,a)$ - how likely are you to end at state $s'$ if you take action $a$ from state $s$. Now there's more than one way to define rewards, but we'll use $r(s,a,s')$ function for convenience.\n",
    "\n",
    "_This notebook is inspired by the awesome_ [CS294](https://github.com/berkeleydeeprlcourse/homework/tree/master/sp17_hw/hw2) _by Berkeley_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starters, let's define a simple MDP from this picture:\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Markov_Decision_Process.svg/800px-Markov_Decision_Process.svg.png' width=300px>\n",
    "_img by MistWiz (Own work) [Public domain], via Wikimedia Commons_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:44:32.642838Z",
     "start_time": "2018-04-02T13:44:32.545142Z"
    }
   },
   "outputs": [],
   "source": [
    "transition_probs = {\n",
    "    's0': {\n",
    "        'a0': {'s0': 0.5, 's2': 0.5},\n",
    "        'a1': {'s2': 1}\n",
    "    },\n",
    "    's1': {\n",
    "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "        'a1': {'s1': 0.95, 's2': 0.05}\n",
    "    },\n",
    "    's2': {\n",
    "        'a0': {'s0': 0.4, 's1': 0.6},\n",
    "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
    "    }\n",
    "}\n",
    "rewards = {\n",
    "    's1': {'a0': {'s0': +5}},\n",
    "    's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "from mdp import MDP\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use MDP just as any other gym environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:44:34.203384Z",
     "start_time": "2018-04-02T13:44:34.199297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state = s0\n",
      "next_state = s2, reward = 0.0, done = False\n"
     ]
    }
   ],
   "source": [
    "print('initial state =', mdp.reset())\n",
    "next_state, reward, done, info = mdp.step('a1')\n",
    "print('next_state = %s, reward = %s, done = %s' % (next_state, reward, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but it also has other methods that you'll need for Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:44:34.956122Z",
     "start_time": "2018-04-02T13:44:34.949856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdp.get_all_states = ('s1', 's0', 's2')\n",
      "mdp.get_possible_actions('s1') =  ('a1', 'a0')\n",
      "mdp.get_next_states('s1', 'a0') =  {'s1': 0.1, 's0': 0.7, 's2': 0.2}\n",
      "mdp.get_reward('s1', 'a0', 's0') =  5\n",
      "mdp.get_transition_prob('s1', 'a0', 's0') =  0.7\n"
     ]
    }
   ],
   "source": [
    "print(\"mdp.get_all_states =\", mdp.get_all_states())\n",
    "print(\"mdp.get_possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
    "print(\"mdp.get_next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
    "print(\"mdp.get_reward('s1', 'a0', 's0') = \", mdp.get_reward('s1', 'a0', 's0'))\n",
    "print(\"mdp.get_transition_prob('s1', 'a0', 's0') = \",\n",
    "      mdp.get_transition_prob('s1', 'a0', 's0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Visualizing MDPs\n",
    "\n",
    "You can also visualize any MDP with the drawing fuction donated by [neer201](https://github.com/neer201).\n",
    "\n",
    "You have to install graphviz for system and for python. For ubuntu just run:\n",
    "\n",
    "1. `sudo apt-get install graphviz`\n",
    "2. `pip install graphviz`\n",
    "3. restart the notebook\n",
    "\n",
    "__Note:__ Installing graphviz on some OS (esp. Windows) may be tricky. However, you can ignore this part alltogether and use the standart vizualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:44:37.797182Z",
     "start_time": "2018-04-02T13:44:37.794073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphviz available: True\n"
     ]
    }
   ],
   "source": [
    "from mdp import has_graphviz\n",
    "from IPython.display import display\n",
    "print(\"Graphviz available:\", has_graphviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:44:38.715883Z",
     "start_time": "2018-04-02T13:44:38.648684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: MDP Pages: 1 -->\n",
       "<svg width=\"720pt\" height=\"182pt\"\n",
       " viewBox=\"0.00 0.00 720.00 181.55\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(0.704871 0.704871) rotate(0) translate(4 253.568)\">\n",
       "<title>MDP</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-253.568 1017.46,-253.568 1017.46,4 -4,4\"/>\n",
       "<!-- s1 -->\n",
       "<g id=\"node1\" class=\"node\"><title>s1</title>\n",
       "<ellipse fill=\"lightgreen\" stroke=\"lightgreen\" cx=\"40\" cy=\"-126.568\" rx=\"36\" ry=\"36\"/>\n",
       "<ellipse fill=\"none\" stroke=\"lightgreen\" cx=\"40\" cy=\"-126.568\" rx=\"40\" ry=\"40\"/>\n",
       "<text text-anchor=\"middle\" x=\"40\" y=\"-120.368\" font-family=\"Arial\" font-size=\"24.00\">s1</text>\n",
       "</g>\n",
       "<!-- s1&#45;a1 -->\n",
       "<g id=\"node2\" class=\"node\"><title>s1&#45;a1</title>\n",
       "<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"201.577\" cy=\"-89.5678\" rx=\"27.6545\" ry=\"27.6545\"/>\n",
       "<text text-anchor=\"middle\" x=\"201.577\" y=\"-84.5678\" font-family=\"Arial\" font-size=\"20.00\">a1</text>\n",
       "</g>\n",
       "<!-- s1&#45;&gt;s1&#45;a1 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>s1&#45;&gt;s1&#45;a1</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M71.1839,-101.036C79.3832,-95.5194 88.607,-90.4737 98,-87.5678 119.301,-80.9781 144.297,-81.3111 164.281,-83.3723\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"163.88,-86.8492 174.229,-84.5802 164.724,-79.9002 163.88,-86.8492\"/>\n",
       "</g>\n",
       "<!-- s1&#45;a0 -->\n",
       "<g id=\"node4\" class=\"node\"><title>s1&#45;a0</title>\n",
       "<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"201.577\" cy=\"-167.568\" rx=\"27.6545\" ry=\"27.6545\"/>\n",
       "<text text-anchor=\"middle\" x=\"201.577\" y=\"-162.568\" font-family=\"Arial\" font-size=\"20.00\">a0</text>\n",
       "</g>\n",
       "<!-- s1&#45;&gt;s1&#45;a0 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>s1&#45;&gt;s1&#45;a0</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M70.8216,-152.476C79.0963,-158.215 88.4466,-163.499 98,-166.568 119.128,-173.355 143.991,-173.772 163.946,-172.412\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"164.517,-175.876 174.181,-171.533 163.918,-168.902 164.517,-175.876\"/>\n",
       "</g>\n",
       "<!-- s1&#45;a1&#45;&gt;s1 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>s1&#45;a1&#45;&gt;s1</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M174.211,-93.0966C153.357,-96.1506 123.592,-101.095 98,-107.568 94.6107,-108.425 91.1318,-109.384 87.645,-110.402\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"86.4018,-107.122 77.8649,-113.397 88.4514,-113.815 86.4018,-107.122\"/>\n",
       "<text text-anchor=\"middle\" x=\"127\" y=\"-112.768\" font-family=\"Arial\" font-size=\"16.00\">p = 0.95</text>\n",
       "</g>\n",
       "<!-- s2 -->\n",
       "<g id=\"node3\" class=\"node\"><title>s2</title>\n",
       "<ellipse fill=\"lightgreen\" stroke=\"lightgreen\" cx=\"363.154\" cy=\"-89.5678\" rx=\"36\" ry=\"36\"/>\n",
       "<ellipse fill=\"none\" stroke=\"lightgreen\" cx=\"363.154\" cy=\"-89.5678\" rx=\"40\" ry=\"40\"/>\n",
       "<text text-anchor=\"middle\" x=\"363.154\" y=\"-83.3678\" font-family=\"Arial\" font-size=\"24.00\">s2</text>\n",
       "</g>\n",
       "<!-- s1&#45;a1&#45;&gt;s2 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>s1&#45;a1&#45;&gt;s2</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M229.287,-89.5678C252.024,-89.5678 285.285,-89.5678 312.84,-89.5678\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"313.088,-93.0679 323.088,-89.5678 313.088,-86.0679 313.088,-93.0679\"/>\n",
       "<text text-anchor=\"middle\" x=\"276.154\" y=\"-94.7678\" font-family=\"Arial\" font-size=\"16.00\">p = 0.05</text>\n",
       "</g>\n",
       "<!-- s2&#45;a1 -->\n",
       "<g id=\"node8\" class=\"node\"><title>s2&#45;a1</title>\n",
       "<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"595.731\" cy=\"-116.568\" rx=\"27.6545\" ry=\"27.6545\"/>\n",
       "<text text-anchor=\"middle\" x=\"595.731\" y=\"-111.568\" font-family=\"Arial\" font-size=\"20.00\">a1</text>\n",
       "</g>\n",
       "<!-- s2&#45;&gt;s2&#45;a1 -->\n",
       "<g id=\"edge13\" class=\"edge\"><title>s2&#45;&gt;s2&#45;a1</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M401.263,-101.984C407.827,-103.792 414.645,-105.426 421.154,-106.568 467.766,-114.747 522.389,-116.658 557.727,-116.912\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"558.034,-120.412 568.045,-116.942 558.055,-113.412 558.034,-120.412\"/>\n",
       "</g>\n",
       "<!-- s2&#45;a0 -->\n",
       "<g id=\"node9\" class=\"node\"><title>s2&#45;a0</title>\n",
       "<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"595.731\" cy=\"-43.5678\" rx=\"27.6545\" ry=\"27.6545\"/>\n",
       "<text text-anchor=\"middle\" x=\"595.731\" y=\"-38.5678\" font-family=\"Arial\" font-size=\"20.00\">a0</text>\n",
       "</g>\n",
       "<!-- s2&#45;&gt;s2&#45;a0 -->\n",
       "<g id=\"edge17\" class=\"edge\"><title>s2&#45;&gt;s2&#45;a0</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M400.993,-76.2903C407.648,-74.1783 414.565,-72.1665 421.154,-70.5678 467.62,-59.2937 522.275,-51.7019 557.661,-47.5107\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"558.461,-50.9418 567.994,-46.3189 557.659,-43.9879 558.461,-50.9418\"/>\n",
       "</g>\n",
       "<!-- s1&#45;a0&#45;&gt;s1 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>s1&#45;a0&#45;&gt;s1</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M177.592,-153.928C170.828,-150.409 163.285,-146.948 156,-144.568 134.966,-137.695 110.886,-133.409 90.214,-130.753\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"90.4034,-127.251 80.0593,-129.543 89.5746,-134.202 90.4034,-127.251\"/>\n",
       "<text text-anchor=\"middle\" x=\"127\" y=\"-149.768\" font-family=\"Arial\" font-size=\"16.00\">p = 0.1</text>\n",
       "</g>\n",
       "<!-- s1&#45;a0&#45;&gt;s2 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>s1&#45;a0&#45;&gt;s2</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M225.393,-152.985C232.282,-148.791 239.935,-144.335 247.154,-140.568 269.486,-128.913 295.038,-117.408 316.376,-108.284\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"318.014,-111.391 325.856,-104.267 315.283,-104.945 318.014,-111.391\"/>\n",
       "<text text-anchor=\"middle\" x=\"276.154\" y=\"-145.768\" font-family=\"Arial\" font-size=\"16.00\">p = 0.2</text>\n",
       "</g>\n",
       "<!-- s0 -->\n",
       "<g id=\"node5\" class=\"node\"><title>s0</title>\n",
       "<ellipse fill=\"lightgreen\" stroke=\"lightgreen\" cx=\"833.309\" cy=\"-116.568\" rx=\"36\" ry=\"36\"/>\n",
       "<ellipse fill=\"none\" stroke=\"lightgreen\" cx=\"833.309\" cy=\"-116.568\" rx=\"40\" ry=\"40\"/>\n",
       "<text text-anchor=\"middle\" x=\"833.309\" y=\"-110.368\" font-family=\"Arial\" font-size=\"24.00\">s0</text>\n",
       "</g>\n",
       "<!-- s1&#45;a0&#45;&gt;s0 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>s1&#45;a0&#45;&gt;s0</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M229.188,-167.283C295.703,-166.441 474.54,-163.373 623.309,-153.568 691.045,-149.103 709.265,-154.266 775.309,-138.568 779.09,-137.669 782.956,-136.577 786.806,-135.365\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"787.939,-138.677 796.284,-132.149 785.69,-132.048 787.939,-138.677\"/>\n",
       "<text text-anchor=\"middle\" x=\"485.654\" y=\"-168.768\" font-family=\"Arial\" font-size=\"16.00\">p = 0.7 &#160;reward =5</text>\n",
       "</g>\n",
       "<!-- s0&#45;a1 -->\n",
       "<g id=\"node6\" class=\"node\"><title>s0&#45;a1</title>\n",
       "<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"985.886\" cy=\"-32.5678\" rx=\"27.6545\" ry=\"27.6545\"/>\n",
       "<text text-anchor=\"middle\" x=\"985.886\" y=\"-27.5678\" font-family=\"Arial\" font-size=\"20.00\">a1</text>\n",
       "</g>\n",
       "<!-- s0&#45;&gt;s0&#45;a1 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>s0&#45;&gt;s0&#45;a1</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M868.477,-97.5069C893.56,-83.5144 927.426,-64.6218 952.163,-50.8225\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"954.131,-53.7324 961.159,-45.804 950.721,-47.6193 954.131,-53.7324\"/>\n",
       "</g>\n",
       "<!-- s0&#45;a0 -->\n",
       "<g id=\"node7\" class=\"node\"><title>s0&#45;a0</title>\n",
       "<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"985.886\" cy=\"-149.568\" rx=\"27.6545\" ry=\"27.6545\"/>\n",
       "<text text-anchor=\"middle\" x=\"985.886\" y=\"-144.568\" font-family=\"Arial\" font-size=\"20.00\">a0</text>\n",
       "</g>\n",
       "<!-- s0&#45;&gt;s0&#45;a0 -->\n",
       "<g id=\"edge10\" class=\"edge\"><title>s0&#45;&gt;s0&#45;a0</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M873.578,-119.196C893.779,-121.188 918.644,-124.639 940.309,-130.568 943.93,-131.559 947.643,-132.782 951.306,-134.125\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"950.202,-137.453 960.788,-137.888 952.784,-130.946 950.202,-137.453\"/>\n",
       "</g>\n",
       "<!-- s0&#45;a1&#45;&gt;s2 -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>s0&#45;a1&#45;&gt;s2</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M958.439,-27.4186C892.355,-15.3101 714.792,12.5534 568.154,-6.56783 501.128,-15.3079 481.771,-15.6591 421.154,-45.5678 414.603,-48.8003 408.072,-52.8498 401.895,-57.1748\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"399.698,-54.4454 393.72,-63.1925 403.848,-60.0827 399.698,-54.4454\"/>\n",
       "<text text-anchor=\"middle\" x=\"708.309\" y=\"-9.76783\" font-family=\"Arial\" font-size=\"16.00\">p = 1</text>\n",
       "</g>\n",
       "<!-- s0&#45;a0&#45;&gt;s2 -->\n",
       "<g id=\"edge12\" class=\"edge\"><title>s0&#45;a0&#45;&gt;s2</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M960.239,-160.069C877.47,-193.587 604.395,-289.875 421.154,-185.568 402.565,-174.986 389.41,-155.688 380.459,-137.155\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"383.576,-135.552 376.276,-127.872 377.194,-138.428 383.576,-135.552\"/>\n",
       "<text text-anchor=\"middle\" x=\"708.309\" y=\"-236.768\" font-family=\"Arial\" font-size=\"16.00\">p = 0.5</text>\n",
       "</g>\n",
       "<!-- s0&#45;a0&#45;&gt;s0 -->\n",
       "<g id=\"edge11\" class=\"edge\"><title>s0&#45;a0&#45;&gt;s0</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M958.89,-143.871C937.921,-139.275 907.85,-132.685 882.506,-127.131\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"883.213,-123.702 872.695,-124.981 881.714,-130.54 883.213,-123.702\"/>\n",
       "<text text-anchor=\"middle\" x=\"915.809\" y=\"-145.768\" font-family=\"Arial\" font-size=\"16.00\">p = 0.5</text>\n",
       "</g>\n",
       "<!-- s2&#45;a1&#45;&gt;s1 -->\n",
       "<g id=\"edge14\" class=\"edge\"><title>s2&#45;a1&#45;&gt;s1</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M587.095,-143.168C580.445,-161.043 568.868,-183.316 550.154,-194.568 426.584,-268.87 291.342,-219.749 174,-204.568 139.361,-200.086 128.211,-202.096 98,-184.568 89.0244,-179.36 80.5041,-172.362 72.9176,-165.031\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"75.1645,-162.324 65.6601,-157.642 70.1707,-167.229 75.1645,-162.324\"/>\n",
       "<text text-anchor=\"middle\" x=\"276.154\" y=\"-230.768\" font-family=\"Arial\" font-size=\"16.00\">p = 0.3</text>\n",
       "</g>\n",
       "<!-- s2&#45;a1&#45;&gt;s2 -->\n",
       "<g id=\"edge16\" class=\"edge\"><title>s2&#45;a1&#45;&gt;s2</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M574.907,-98.3138C567.657,-92.7872 559.042,-87.4275 550.154,-84.5678 505.322,-70.1423 451.228,-73.7692 412.962,-79.5654\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"411.92,-76.1886 402.609,-81.2446 413.041,-83.0983 411.92,-76.1886\"/>\n",
       "<text text-anchor=\"middle\" x=\"485.654\" y=\"-89.7678\" font-family=\"Arial\" font-size=\"16.00\">p = 0.4</text>\n",
       "</g>\n",
       "<!-- s2&#45;a1&#45;&gt;s0 -->\n",
       "<g id=\"edge15\" class=\"edge\"><title>s2&#45;a1&#45;&gt;s0</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M623.755,-116.568C662.488,-116.568 734.444,-116.568 782.856,-116.568\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"783.11,-120.068 793.11,-116.568 783.11,-113.068 783.11,-120.068\"/>\n",
       "<text text-anchor=\"middle\" x=\"708.309\" y=\"-121.768\" font-family=\"Arial\" font-size=\"16.00\">p = 0.3 &#160;reward =&#45;1</text>\n",
       "</g>\n",
       "<!-- s2&#45;a0&#45;&gt;s1 -->\n",
       "<g id=\"edge18\" class=\"edge\"><title>s2&#45;a0&#45;&gt;s1</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M568.261,-40.8222C501.335,-34.5903 320.281,-22.1601 174,-52.5678 139.047,-59.8335 129.073,-60.992 98,-78.5678 90.9533,-82.5536 83.9456,-87.459 77.3791,-92.5962\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"74.8564,-90.1376 69.3276,-99.1755 79.2858,-95.5581 74.8564,-90.1376\"/>\n",
       "<text text-anchor=\"middle\" x=\"276.154\" y=\"-46.7678\" font-family=\"Arial\" font-size=\"16.00\">p = 0.6</text>\n",
       "</g>\n",
       "<!-- s2&#45;a0&#45;&gt;s0 -->\n",
       "<g id=\"edge19\" class=\"edge\"><title>s2&#45;a0&#45;&gt;s0</title>\n",
       "<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M622.684,-50.1928C657.638,-59.2943 721.654,-76.5925 775.309,-94.5678 778.994,-95.8025 782.792,-97.1344 786.595,-98.5104\"/>\n",
       "<polygon fill=\"blue\" stroke=\"blue\" points=\"785.404,-101.802 795.997,-101.991 787.834,-95.237 785.404,-101.802\"/>\n",
       "<text text-anchor=\"middle\" x=\"708.309\" y=\"-99.7678\" font-family=\"Arial\" font-size=\"16.00\">p = 0.4</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f11944ef0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if has_graphviz:\n",
    "    from mdp import plot_graph, plot_graph_with_state_values, \\\n",
    "        plot_graph_optimal_strategy_and_state_values\n",
    "\n",
    "    display(plot_graph(mdp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "Now let's build something to solve this MDP. The simplest algorithm so far is __V__alue __I__teration\n",
    "\n",
    "Here's the pseudo-code for VI:\n",
    "\n",
    "---\n",
    "\n",
    "`1.` Initialize $V^{(0)}(s)=0$, for all $s$\n",
    "\n",
    "`2.` For $i=0, 1, 2, \\dots$\n",
    " \n",
    "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, for all $s$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's write a function to compute the state-action value function $Q^{\\pi}$, defined as follows\n",
    "\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.101416Z",
     "start_time": "2018-04-02T13:43:17.095468Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_action_value(mdp, state_values, state, action, gamma):\n",
    "    \"\"\" Computes Q(s,a) as in formula above \"\"\"\n",
    "\n",
    "    q_s_a = sum([mdp.get_transition_prob(state, action, next_state)*\n",
    "                 (mdp.get_reward(state, action, next_state) + gamma*state_values[next_state]) \n",
    "                 for next_state in state_values.keys()])\n",
    "\n",
    "    return q_s_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.102247Z",
     "start_time": "2018-04-02T13:43:05.502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s1': 1, 's0': 0, 's2': 2}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_Vs = {s: i for i, s in enumerate(sorted(mdp.get_all_states()))}\n",
    "print(test_Vs)\n",
    "assert np.allclose(get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)\n",
    "assert np.allclose(get_action_value(mdp, test_Vs, 's1', 'a0', 0.9), 3.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $Q(s,a)$ we can now define the \"next\" V(s) for value iteration.\n",
    " $$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a1', 'a0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp.get_possible_actions('s0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.103358Z",
     "start_time": "2018-04-02T13:43:05.506Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\" Computes next V(s) as in formula above. Please do not change state_values in process. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return 0\n",
    "\n",
    "    v_i_1 = max([get_action_value(mdp, state_values, state, a, gamma) \n",
    "                 for a in mdp.get_possible_actions(state)])\n",
    "    return v_i_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.104340Z",
     "start_time": "2018-04-02T13:43:05.510Z"
    }
   },
   "outputs": [],
   "source": [
    "test_Vs_copy = dict(test_Vs)\n",
    "assert np.allclose(get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)\n",
    "assert np.allclose(get_new_state_value(mdp, test_Vs, 's2', 0.9), 0.69)\n",
    "assert test_Vs == test_Vs_copy, \"please do not change state_values in get_new_state_value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's combine everything we wrote into a working value iteration algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:44:09.793405Z",
     "start_time": "2018-04-02T13:44:09.770623Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "gamma = 0.9            # discount for MDP\n",
    "num_iter = 100         # maximum iterations, excluding initialization\n",
    "# stop VI if new values are this close to old values (or closer)\n",
    "min_difference = 0.001\n",
    "\n",
    "# initialize V(s)\n",
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "\n",
    "if has_graphviz:\n",
    "    display(plot_graph_with_state_values(mdp, state_values))\n",
    "\n",
    "for i in range(num_iter):\n",
    "\n",
    "    # Compute new state values using the functions you defined above.\n",
    "    # It must be a dict {state : float V_new(state)}\n",
    "    new_state_values = <YOUR CODE HERE >\n",
    "\n",
    "    assert isinstance(new_state_values, dict)\n",
    "\n",
    "    # Compute difference\n",
    "    diff = max(abs(new_state_values[s] - state_values[s])\n",
    "               for s in mdp.get_all_states())\n",
    "    print(\"iter %4i   |   diff: %6.5f   |   \" % (i, diff), end=\"\")\n",
    "    print('   '.join(\"V(%s) = %.3f\" % (s, v)\n",
    "                     for s, v in state_values.items()), end='\\n\\n')\n",
    "    state_values = new_state_values\n",
    "\n",
    "    if diff < min_difference:\n",
    "        print(\"Terminated\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if has_graphviz:\n",
    "    display(plot_graph_with_state_values(mdp, state_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.106395Z",
     "start_time": "2018-04-02T13:43:05.522Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Final state values:\", state_values)\n",
    "\n",
    "assert abs(state_values['s0'] - 8.032) < 0.01\n",
    "assert abs(state_values['s1'] - 11.169) < 0.01\n",
    "assert abs(state_values['s2'] - 8.921) < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use those $V^{*}(s)$ to find optimal actions in each state\n",
    "\n",
    " $$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$\n",
    " \n",
    "The only difference vs V(s) is that here we take not max but argmax: find action such with maximum Q(s,a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.107338Z",
     "start_time": "2018-04-02T13:43:05.525Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return None\n",
    "\n",
    "    # <YOUR CODE HERE>\n",
    "\n",
    "    return < YOUR CODE >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.108149Z",
     "start_time": "2018-04-02T13:43:05.530Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert get_optimal_action(mdp, state_values, 's0', gamma) == 'a1'\n",
    "assert get_optimal_action(mdp, state_values, 's1', gamma) == 'a0'\n",
    "assert get_optimal_action(mdp, state_values, 's2', gamma) == 'a0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:44:05.017823Z",
     "start_time": "2018-04-02T13:44:04.962755Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if has_graphviz:\n",
    "    display(plot_optimal_strategy_and_state_values(mdp, state_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.110002Z",
     "start_time": "2018-04-02T13:43:05.538Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "\n",
    "s = mdp.reset()\n",
    "rewards = []\n",
    "for _ in range(10000):\n",
    "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "    rewards.append(r)\n",
    "\n",
    "print(\"average reward: \", np.mean(rewards))\n",
    "\n",
    "assert(0.85 < np.mean(rewards) < 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.110991Z",
     "start_time": "2018-04-02T13:43:05.541Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mdp import FrozenLakeEnv\n",
    "mdp = FrozenLakeEnv(slip_chance=0)\n",
    "\n",
    "mdp.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.111919Z",
     "start_time": "2018-04-02T13:43:05.545Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration(mdp, state_values=None, gamma=0.9, num_iter=1000, min_difference=1e-5):\n",
    "    \"\"\" performs num_iter value iteration steps starting from state_values. Same as before but in a function \"\"\"\n",
    "    state_values = state_values or {s: 0 for s in mdp.get_all_states()}\n",
    "    for i in range(num_iter):\n",
    "\n",
    "        # Compute new state values using the functions you defined above. It must be a dict {state : new_V(state)}\n",
    "        new_state_values = <YOUR CODE HERE >\n",
    "\n",
    "        assert isinstance(new_state_values, dict)\n",
    "\n",
    "        # Compute difference\n",
    "        diff = max(abs(new_state_values[s] - state_values[s])\n",
    "                   for s in mdp.get_all_states())\n",
    "\n",
    "        print(\"iter %4i   |   diff: %6.5f   |   V(start): %.3f \" %\n",
    "              (i, diff, new_state_values[mdp._initial_state]))\n",
    "\n",
    "        state_values = new_state_values\n",
    "        if diff < min_difference:\n",
    "            break\n",
    "\n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.112871Z",
     "start_time": "2018-04-02T13:43:05.548Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_values = value_iteration(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.114062Z",
     "start_time": "2018-04-02T13:43:05.552Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = mdp.reset()\n",
    "mdp.render()\n",
    "for t in range(100):\n",
    "    a = get_optimal_action(mdp, state_values, s, gamma)\n",
    "    print(a, end='\\n\\n')\n",
    "    s, r, done, _ = mdp.step(a)\n",
    "    mdp.render()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize!\n",
    "\n",
    "It's usually interesting to see what your algorithm actually learned under the hood. To do so, we'll plot state value functions and optimal actions at each VI step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.115092Z",
     "start_time": "2018-04-02T13:43:05.556Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def draw_policy(mdp, state_values):\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    h, w = mdp.desc.shape\n",
    "    states = sorted(mdp.get_all_states())\n",
    "    V = np.array([state_values[s] for s in states])\n",
    "    Pi = {s: get_optimal_action(mdp, state_values, s, gamma) for s in states}\n",
    "    plt.imshow(V.reshape(w, h), cmap='gray', interpolation='none', clim=(0, 1))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(h)-.5)\n",
    "    ax.set_yticks(np.arange(w)-.5)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    Y, X = np.mgrid[0:4, 0:4]\n",
    "    a2uv = {'left': (-1, 0), 'down': (0, -1), 'right': (1, 0), 'up': (-1, 0)}\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            plt.text(x, y, str(mdp.desc[y, x].item()),\n",
    "                     color='g', size=12,  verticalalignment='center',\n",
    "                     horizontalalignment='center', fontweight='bold')\n",
    "            a = Pi[y, x]\n",
    "            if a is None:\n",
    "                continue\n",
    "            u, v = a2uv[a]\n",
    "            plt.arrow(x, y, u*.3, -v*.3, color='m',\n",
    "                      head_width=0.1, head_length=0.1)\n",
    "    plt.grid(color='b', lw=2, ls='-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.116164Z",
     "start_time": "2018-04-02T13:43:05.560Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"after iteration %i\" % i)\n",
    "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
    "    draw_policy(mdp, state_values)\n",
    "# please ignore iter 0 at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.117143Z",
     "start_time": "2018-04-02T13:43:05.563Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "mdp = FrozenLakeEnv(map_name='8x8', slip_chance=0.1)\n",
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "\n",
    "for i in range(30):\n",
    "    clear_output(True)\n",
    "    print(\"after iteration %i\" % i)\n",
    "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
    "    draw_policy(mdp, state_values)\n",
    "    sleep(0.5)\n",
    "# please ignore iter 0 at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Massive tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.118218Z",
     "start_time": "2018-04-02T13:43:05.568Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mdp = FrozenLakeEnv(slip_chance=0)\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(\n",
    "            get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "\n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(1.0 <= np.mean(total_rewards) <= 1.0)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.119075Z",
     "start_time": "2018-04-02T13:43:05.571Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.1)\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(\n",
    "            get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "\n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.8 <= np.mean(total_rewards) <= 0.95)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.120316Z",
     "start_time": "2018-04-02T13:43:05.574Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.25)\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(\n",
    "            get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "\n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.6 <= np.mean(total_rewards) <= 0.7)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.121544Z",
     "start_time": "2018-04-02T13:43:05.578Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.2, map_name='8x8')\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(\n",
    "            get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "\n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.6 <= np.mean(total_rewards) <= 0.8)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus 1 - find an MDP for which value iteration takes long to converge  (2+ pts)\n",
    "\n",
    "When we ran value iteration on the small frozen lake problem, the last iteration where an action changed was iteration 6--i.e., value iteration computed the optimal policy at iteration 6. Are there any guarantees regarding how many iterations it'll take value iteration to compute the optimal policy? There are no such guarantees without additional assumptions--we can construct the MDP in such a way that the greedy policy will change after arbitrarily many iterations.\n",
    "\n",
    "Your task: define an MDP with at most 3 states and 2 actions, such that when you run value iteration, the optimal action changes at iteration >= 50. Use discount=0.95. (However, note that the discount doesn't matter here--you can construct an appropriate MDP with any discount.)\n",
    "\n",
    "Note: value function must change at least once after iteration >=50, not necessarily change on every iteration till >=50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.122424Z",
     "start_time": "2018-04-02T13:43:05.582Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transition_probs = {\n",
    "    < YOUR CODE >\n",
    "}\n",
    "rewards = {\n",
    "    < YOUR CODE >\n",
    "}\n",
    "\n",
    "from mdp import MDP\n",
    "mdp = MDP(transition_probs, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.123825Z",
     "start_time": "2018-04-02T13:43:05.586Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "policy = [get_optimal_action(mdp, state_values, state, gamma)\n",
    "          for state in sorted(mdp.get_all_states())]\n",
    "\n",
    "for i in range(100):\n",
    "    print(\"after iteration %i\" % i)\n",
    "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
    "\n",
    "    new_policy = [get_optimal_action(mdp, state_values, state, gamma)\n",
    "                  for state in sorted(mdp.get_all_states())]\n",
    "\n",
    "    n_changes = np.not_equal(policy, new_policy).sum()\n",
    "    print(\"N actions changed = %i \\n\" % n_changes)\n",
    "    policy = new_policy\n",
    "\n",
    "# please ignore iter 0 at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus 2 - Policy Iteration (3+ points)\n",
    "\n",
    "Let's implement exact policy iteration (PI), which has the following pseudocode:\n",
    "\n",
    "---\n",
    "Initialize $\\pi_0$   `// random or fixed action`\n",
    "\n",
    "For $n=0, 1, 2, \\dots$\n",
    "- Compute the state-value function $V^{\\pi_{n}}$\n",
    "- Using $V^{\\pi_{n}}$, compute the state-action-value function $Q^{\\pi_{n}}$\n",
    "- Compute new policy $\\pi_{n+1}(s) = \\operatorname*{argmax}_a Q^{\\pi_{n}}(s,a)$\n",
    "---\n",
    "\n",
    "Unlike VI, policy iteration has to maintain a policy - chosen actions from all states - and estimate $V^{\\pi_{n}}$ based on this policy. It only changes policy once values converged.\n",
    "\n",
    "\n",
    "Below are a few helpers that you may or may not use in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.125320Z",
     "start_time": "2018-04-02T13:43:05.590Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transition_probs = {\n",
    "    's0': {\n",
    "        'a0': {'s0': 0.5, 's2': 0.5},\n",
    "        'a1': {'s2': 1}\n",
    "    },\n",
    "    's1': {\n",
    "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "        'a1': {'s1': 0.95, 's2': 0.05}\n",
    "    },\n",
    "    's2': {\n",
    "        'a0': {'s0': 0.4, 's1': 0.6},\n",
    "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
    "    }\n",
    "}\n",
    "rewards = {\n",
    "    's1': {'a0': {'s0': +5}},\n",
    "    's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "from mdp import MDP\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function called `compute_vpi` that computes the state-value function $V^{\\pi}$ for an arbitrary policy $\\pi$.\n",
    "\n",
    "Unlike VI, this time you must find the exact solution, not just a single iteration.\n",
    "\n",
    "Recall that $V^{\\pi}$ satisfies the following linear equation:\n",
    "$$V^{\\pi}(s) = \\sum_{s'} P(s,\\pi(s),s')[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$$\n",
    "\n",
    "You'll have to solve a linear system in your code. (Find an exact solution, e.g., with `np.linalg.solve`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.126518Z",
     "start_time": "2018-04-02T13:43:05.593Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_vpi(mdp, policy, gamma):\n",
    "    \"\"\"\n",
    "    Computes V^pi(s) FOR ALL STATES under given policy.\n",
    "    :param policy: a dict of currently chosen actions {s : a}\n",
    "    :returns: a dict {state : V^pi(state) for all states}\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return < YOUR CODE >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.127349Z",
     "start_time": "2018-04-02T13:43:05.597Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_policy = {s: np.random.choice(\n",
    "    mdp.get_possible_actions(s)) for s in mdp.get_all_states()}\n",
    "new_vpi = compute_vpi(mdp, test_policy, gamma)\n",
    "\n",
    "print(new_vpi)\n",
    "\n",
    "assert type(\n",
    "    new_vpi) is dict, \"compute_vpi must return a dict {state : V^pi(state) for all states}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've got new state values, it's time to update our policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.128415Z",
     "start_time": "2018-04-02T13:43:05.601Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_new_policy(mdp, vpi, gamma):\n",
    "    \"\"\"\n",
    "    Computes new policy as argmax of state values\n",
    "    :param vpi: a dict {state : V^pi(state) for all states}\n",
    "    :returns: a dict {state : optimal action for all states}\n",
    "    \"\"\"\n",
    "    <YOUR CODE >\n",
    "    return < YOUR CODE >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.129416Z",
     "start_time": "2018-04-02T13:43:05.604Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_policy = compute_new_policy(mdp, new_vpi, gamma)\n",
    "\n",
    "print(new_policy)\n",
    "\n",
    "assert type(\n",
    "    new_policy) is dict, \"compute_new_policy must return a dict {state : optimal action for all states}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Main loop__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.130183Z",
     "start_time": "2018-04-02T13:43:05.608Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_iteration(mdp, policy=None, gamma=0.9, num_iter=1000, min_difference=1e-5):\n",
    "    \"\"\" \n",
    "    Run the policy iteration loop for num_iter iterations or till difference between V(s) is below min_difference.\n",
    "    If policy is not given, initialize it at random.\n",
    "    \"\"\"\n",
    "    < A WHOLE LOT OF YOUR CODE >\n",
    "\n",
    "    return state_values, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your PI Results__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T13:43:17.130926Z",
     "start_time": "2018-04-02T13:43:05.612Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "< Compare PI and VI on the MDP from bonus 1, then on small & large FrozenLake >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
