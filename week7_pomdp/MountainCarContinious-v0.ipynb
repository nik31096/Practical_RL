{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-critic for MountainCarContinuous-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_shape = env.observation_space.shape\n",
    "action_shape = env.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.dense1 = nn.Linear(obs_shape[0], 200) \n",
    "        self.dense2 = nn.Linear(200, 1)\n",
    "        self.dense3 = nn.Linear(200, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        mu = self.dense2(x)\n",
    "        sigma = self.dense3(x)\n",
    "        \n",
    "        mu = torch.squeeze(mu)\n",
    "        sigma = F.softplus(torch.squeeze(sigma)) + 1e-5\n",
    "        self.normal = torch.distributions.normal.Normal(mu, sigma)\n",
    "        actions = self.normal.sample(sample_shape=torch.Size(action_shape))\n",
    "        actions = torch.clamp(actions, env.action_space.low[0], env.action_space.high[0])\n",
    "        return actions\n",
    "    \n",
    "    def get_entropy(self):\n",
    "        return self.normal.entropy()\n",
    "    \n",
    "    def get_log_prob(self, action):\n",
    "        return self.normal.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.dense1 = nn.Linear(obs_shape[0], 200)\n",
    "        self.dense2 = nn.Linear(200, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        v_s = self.dense2(x)\n",
    "        \n",
    "        return v_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_estimator = PolicyNetwork()\n",
    "policy_estimator([s]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_estimator = ValueNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(tmax=1000):\n",
    "    states, actions, rewards, dones, next_states = [], [], [], [], []\n",
    "    s = env.reset()\n",
    "    for i in range(tmax):\n",
    "        action = policy_estimator([s])\n",
    "        new_s, reward, done, info = env.step(action.data.numpy())\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        states.append(s)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        next_states.append(new_s)\n",
    "        s = new_s\n",
    "        \n",
    "    return states, actions, rewards, dones, next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards, dones, next_states = generate_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(states, rewards, next_states, gamma):\n",
    "    next_v_s = value_estimator(next_states).detach()\n",
    "    td_target = rewards + gamma*next_v_s\n",
    "    td_error = (td_target - value_estimator(states))**2\n",
    "    loss = torch.mean(td_error)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_J_hat(states, actions, rewards, next_states, gamma):\n",
    "    H = -policy_estimator.get_entropy()\n",
    "    td_target = rewards + gamma*value_estimator(next_states)\n",
    "    J_hat = -policy_estimator.get_log_prob(actions)*(td_target - value_estimator(states)).detach()\n",
    "    loss = H + J_hat\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(gamma=0.98, episodes=10, tmax=1000):\n",
    "    Ltd_opt = torch.optim.Adam(policy_estimator.parameters(), lr=0.001)\n",
    "    J_opt = torch.optim.Adam(value_estimator.parameters(), lr=0.001)\n",
    "    rewards = []\n",
    "    loss_Ltd = []\n",
    "    loss_J = []\n",
    "    for i_episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_rewards = []\n",
    "        \n",
    "        for t in range(tmax):\n",
    "            \n",
    "            action = policy_estimator(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "            \n",
    "            Ltd = compute_td_loss(state, reward, next_state, gamma)\n",
    "            if t % 100 == 0:\n",
    "                loss_Ltd.append(Ltd)\n",
    "            Ltd.backward()\n",
    "            Ltd_opt.step()\n",
    "            Ltd_opt.zero_grad()\n",
    "            \n",
    "            J = compute_J_hat(state, action, reward, next_state, gamma)\n",
    "            if t % 100 == 0:\n",
    "                loss_J.append(J)\n",
    "            J.backward()\n",
    "            J_opt.step()\n",
    "            J_opt.zero_grad()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards.append(np.mean(np.array(episode_rewards)))\n",
    "        print(\"episode: {}, reward: {}\".format(i_episode + 1, np.mean(np.array(episode_rewards))))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(loss_Ltd)\n",
    "    plt.title(\"Ltd loss\")\n",
    "    plt.subplot(122)\n",
    "    plt.plot(loss_J)\n",
    "    plt.title(\"J_hat loss\")\n",
    "    plt.show()\n",
    "        \n",
    "    return np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
