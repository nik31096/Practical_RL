{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from IPython.core import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "# If you are running on a server, launch xvfb to record game videos\n",
    "# Please make sure you have xvfb installed\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY = : 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kung-Fu, recurrent style\n",
    "\n",
    "In this notebook we'll once again train RL agent for for atari [KungFuMaster](https://gym.openai.com/envs/KungFuMaster-v0/), this time using recurrent neural networks.\n",
    "\n",
    "![http://www.retroland.com/wp-content/uploads/2011/07/King-Fu-Master.jpg](http://www.retroland.com/wp-content/uploads/2011/07/King-Fu-Master.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape: (1, 42, 42)\n",
      "Num actions: 14\n",
      "Action names: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik-96/anaconda3/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from atari_util import PreprocessAtari\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"KungFuMasterDeterministic-v0\")\n",
    "    env = PreprocessAtari(env, height=42, width=42,\n",
    "                          crop=lambda img: img[60:-30, 15:],\n",
    "                          color=False, n_frames=1)\n",
    "    return env\n",
    "\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"Observation shape:\", obs_shape)\n",
    "print(\"Num actions:\", n_actions)\n",
    "print(\"Action names:\", env.env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "for _ in range(100):\n",
    "    s, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.title('Game image')\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.show()\n",
    "\n",
    "plt.title('Agent observation')\n",
    "plt.imshow(s.reshape([42, 42]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POMDP setting\n",
    "\n",
    "The atari game we're working with is actually a POMDP: your agent needs to know timing at which enemies spawn and move, but cannot do so unless it has some memory. \n",
    "\n",
    "Let's design another agent that has a recurrent neural net memory to solve this. Here's a sketch.\n",
    "\n",
    "![img](img1.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# a special module that converts [batch, channel, w, h] to [batch, units]\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRecurrentAgent(nn.Module):\n",
    "    def __init__(self, obs_shape, n_actions, reuse=False):\n",
    "        \"\"\"A simple actor-critic agent\"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "\n",
    "        self.conv0 = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.conv1 = nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        self.hid = nn.Linear(512, 128)\n",
    "        #              input_size | hidden_size\n",
    "        self.rnn = nn.LSTMCell(128, 128)\n",
    "\n",
    "        self.logits = nn.Linear(128, n_actions)\n",
    "        self.state_value = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, prev_state, obs_t):\n",
    "        \"\"\"\n",
    "        Takes agent's previous step and observation, \n",
    "        returns next state and whatever it needs to learn (tf tensors)\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE: apply the whole neural net for one step here.\n",
    "        # See docs on self.rnn(...)\n",
    "        # the recurrent cell should take the last feedforward dense layer as input\n",
    "        obs_t = torch.tensor(obs_t, dtype=torch.float32)\n",
    "        x = self.conv0(obs_t)\n",
    "        x = nn.ELU()(x)\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ELU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ELU()(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.hid(x)\n",
    "        #                                                 \n",
    "        new_state = self.rnn(x, prev_state) # new_state = (h_1, c_1)\n",
    "        \n",
    "        logits = self.logits(new_state[0])\n",
    "        state_value = self.state_value(new_state[0])\n",
    "\n",
    "        return new_state, (logits, state_value)\n",
    "\n",
    "    def get_initial_state(self, batch_size):\n",
    "        \"\"\"Return a list of agent memory states at game start. Each state is a np array of shape [batch_size, ...]\"\"\"\n",
    "        return (Variable(torch.zeros((batch_size, 128))),\n",
    "                Variable(torch.zeros((batch_size, 128))))\n",
    "\n",
    "    def sample_actions(self, agent_outputs):\n",
    "        \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"\n",
    "        logits, state_values = agent_outputs\n",
    "        probs = F.softmax(logits)\n",
    "        return torch.multinomial(probs, 1)[:, 0].data.numpy()\n",
    "\n",
    "    def step(self, prev_state, obs_t):\n",
    "        \"\"\" like forward, but obs_t is not Variable \"\"\"\n",
    "        obs_t = Variable(torch.FloatTensor(np.array(obs_t)))\n",
    "        (h, c), (l, s) = self.forward(prev_state, obs_t)\n",
    "        return (h.detach(), c.detach()), (l.detach(), s.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parallel_games = 5\n",
    "gamma = 0.99\n",
    "\n",
    "agent = SimpleRecurrentAgent(obs_shape, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action logits:\n",
      " tensor([[ 7.0828e-02,  9.7533e-02, -6.8679e-02, -6.6385e-02, -7.3680e-02,\n",
      "         -7.6938e-02, -9.5488e-02,  7.5982e-06,  1.5176e-02,  8.0131e-02,\n",
      "         -4.2818e-02, -2.7729e-02,  8.1900e-02,  7.4574e-02]])\n",
      "state values:\n",
      " tensor([[0.0353]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik-96/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "state = [env.reset()]\n",
    "_, (logits, value) = agent.step(agent.get_initial_state(1), state)\n",
    "print(\"action logits:\\n\", logits)\n",
    "print(\"state values:\\n\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play!\n",
    "Let's build a function that measures agent's average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, n_games=1):\n",
    "    \"\"\"Plays an entire game start to end, returns session rewards.\"\"\"\n",
    "\n",
    "    game_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        # initial observation and memory\n",
    "        observation = env.reset()\n",
    "        prev_memories = agent.get_initial_state(1)\n",
    "\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            new_memories, readouts = agent.step(\n",
    "                prev_memories, observation[None, ...])\n",
    "            action = agent.sample_actions(readouts)\n",
    "\n",
    "            observation, reward, done, info = env.step(action[0])\n",
    "\n",
    "            total_reward += reward\n",
    "            prev_memories = new_memories\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        game_rewards.append(total_reward)\n",
    "    return game_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik-96/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/nik-96/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/nik-96/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/nik-96/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300.0, 700.0, 200.0]\n"
     ]
    }
   ],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "rw = evaluate(agent, env_monitor, n_games=3,)\n",
    "env_monitor.close()\n",
    "print(rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./kungfu_videos/openaigym.video.0.24631.video000000.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s: s.endswith(\n",
    "    \".mp4\"), os.listdir(\"./kungfu_videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on parallel games\n",
    "\n",
    "We introduce a class called EnvPool - it's a tool that handles multiple environments for you. Here's how it works:\n",
    "![img](img2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik-96/anaconda3/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "from env_pool import EnvPool\n",
    "pool = EnvPool(agent, make_env, n_parallel_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gonna train our agent on a thing called __rollouts:__\n",
    "![img](img3.jpg)\n",
    "\n",
    "A rollout is just a sequence of T observations, actions and rewards that agent took consequently.\n",
    "* First __s0__ is not necessarily initial state for the environment\n",
    "* Final state is not necessarily terminal\n",
    "* We sample several parallel rollouts for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik-96/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/nik-96/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# for each of n_parallel_games, take 10 steps\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions shape: (5, 10)\n",
      "Rewards shape: (5, 10)\n",
      "Mask shape: (5, 10)\n",
      "Observations shape:  (5, 10, 1, 42, 42)\n"
     ]
    }
   ],
   "source": [
    "print(\"Actions shape:\", rollout_actions.shape)\n",
    "print(\"Rewards shape:\", rollout_rewards.shape)\n",
    "print(\"Mask shape:\", rollout_mask.shape)\n",
    "print(\"Observations shape: \", rollout_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic objective\n",
    "\n",
    "Here we define a loss function that uses rollout above to train advantage actor-critic agent.\n",
    "\n",
    "\n",
    "Our loss consists of three components:\n",
    "\n",
    "* __The policy \"loss\"__\n",
    " $$ \\hat J = {1 \\over T} \\cdot \\sum_t { \\log \\pi(a_t | s_t) } \\cdot A_{const}(s,a) $$\n",
    "  * This function has no meaning in and of itself, but it was built such that\n",
    "  * $ \\nabla \\hat J = {1 \\over T} \\cdot \\sum_t { \\nabla \\log \\pi(a_t | s_t) } \\cdot A(s,a) \\approx \\nabla E_{s, a \\sim \\pi} R(s,a) $\n",
    "  * Therefore if we __maximize__ J_hat with gradient descent we will maximize expected reward\n",
    "  \n",
    "  \n",
    "* __The value \"loss\"__\n",
    "  $$ L_{td} = {1 \\over T} \\cdot \\sum_t { [r + \\gamma \\cdot V_{const}(s_{t+1}) - V(s_t)] ^ 2 }$$\n",
    "  * Ye Olde TD_loss from q-learning and alike\n",
    "  * If we minimize this loss, V(s) will converge to $V_\\pi(s) = E_{a \\sim \\pi(a | s)} R(s,a) $\n",
    "\n",
    "\n",
    "* __Entropy Regularizer__\n",
    "  $$ H = - {1 \\over T} \\sum_t \\sum_a {\\pi(a|s_t) \\cdot \\log \\pi (a|s_t)}$$\n",
    "  * If we __maximize__ entropy we discourage agent from predicting zero probability to actions\n",
    "  prematurely (a.k.a. exploration)\n",
    "  \n",
    "  \n",
    "So we optimize a linear combination of $L_{td}$ $- \\hat J$, $-H$\n",
    "  \n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "__One more thing:__ since we train on T-step rollouts, we can use N-step formula for advantage for free:\n",
    "  * At the last step, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot V(s_{t+1}) - V(s) $\n",
    "  * One step earlier, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot r(s_{t+1}, a_{t+1}) + \\gamma ^ 2 \\cdot V(s_{t+2}) - V(s) $\n",
    "  * Et cetera, et cetera. This way agent starts training much faster since it's estimate of A(s,a) depends less on his (imperfect) value function and more on actual rewards. There's also a [nice generalization](https://arxiv.org/abs/1506.02438) of this.\n",
    "\n",
    "\n",
    "__Note:__ it's also a good idea to scale rollout_len up to learn longer sequences. You may wish set it to >=20 or to start at 10 and then scale up as time passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take an integer vector (tensor of variable) and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y.data if isinstance(y, Variable) else y\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(\n",
    "        y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    return Variable(y_one_hot) if isinstance(y, Variable) else y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "def train_on_rollout(states, actions, rewards, is_not_done, prev_memory_states, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # cast everything into a variable\n",
    "    # shape: [batch_size, time, c, h, w]\n",
    "    states = Variable(torch.FloatTensor(np.array(states)))\n",
    "    actions = Variable(torch.IntTensor(np.array(actions))\n",
    "                       )   # shape: [batch_size, time]\n",
    "    rewards = Variable(torch.FloatTensor(np.array(rewards))\n",
    "                       )  # shape: [batch_size, time]\n",
    "    is_not_done = Variable(torch.FloatTensor(\n",
    "        is_not_done.astype('float32')))  # shape: [batch_size, time]\n",
    "    rollout_length = rewards.shape[1] - 1\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    memory = [m.detach() for m in prev_memory_states]\n",
    "\n",
    "    logits = []  # append logit sequence here\n",
    "    state_values = []  # append state values here\n",
    "    for t in range(rewards.shape[1]):\n",
    "        obs_t = states[:, t]\n",
    "\n",
    "        # use agent to comute logits_t and state values_t.\n",
    "        # append them to logits and state_values array\n",
    "\n",
    "        memory, (logits_t, values_t) = <YOUR CODE >\n",
    "\n",
    "        logits.append(logits_t)\n",
    "        state_values.append(values_t)\n",
    "\n",
    "    logits = torch.stack(logits, dim=1)\n",
    "    state_values = torch.stack(state_values, dim=1)\n",
    "    probas = F.softmax(logits, dim=2)\n",
    "    logprobas = F.log_softmax(logits, dim=2)\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    actions_one_hot = to_one_hot(actions, n_actions).view(\n",
    "        actions.shape[0], actions.shape[1], n_actions)\n",
    "    logprobas_for_actions = torch.sum(logprobas * actions_one_hot, dim=-1)\n",
    "\n",
    "    # Now let's compute two loss components:\n",
    "    # 1) Policy gradient objective.\n",
    "    # Notes: Please don't forget to call .detach() on advantage term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    J_hat = 0  # policy objective as in the formula for J_hat\n",
    "\n",
    "    # 2) Temporal difference MSE for state values\n",
    "    # Notes: Please don't forget to call on V(s') term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    value_loss = 0\n",
    "\n",
    "    cumulative_returns = state_values[:, -1].detach()\n",
    "\n",
    "    for t in reversed(range(rollout_length)):\n",
    "        r_t = rewards[:, t]                                # current rewards\n",
    "        # current state values\n",
    "        V_t = state_values[:, t]\n",
    "        V_next = state_values[:, t + 1].detach()           # next state values\n",
    "        # log-probability of a_t in s_t\n",
    "        logpi_a_s_t = logprobas_for_actions[:, t]\n",
    "\n",
    "        # update G_t = r_t + gamma * G_{t+1} as we did in week6 reinforce\n",
    "        cumulative_returns = G_t = r_t + gamma * cumulative_returns\n",
    "\n",
    "        # Compute temporal difference error (MSE for V(s))\n",
    "        value_loss += <YOUR CODE >\n",
    "\n",
    "        # compute advantage A(s_t, a_t) using cumulative returns and V(s_t) as baseline\n",
    "        advantage = <YOUR CODE >\n",
    "        advantage = advantage.detach()\n",
    "\n",
    "        # compute policy pseudo-loss aka -J_hat.\n",
    "        J_hat += <YOUR CODE >\n",
    "\n",
    "    # regularize with entropy\n",
    "    entropy_reg = <compute entropy regularizer >\n",
    "\n",
    "    # add-up three loss components and average over time\n",
    "    loss = -J_hat / rollout_length +\\\n",
    "        value_loss / rollout_length +\\\n",
    "           -0.01 * entropy_reg\n",
    "\n",
    "    # Gradient descent step\n",
    "    < your code >\n",
    "\n",
    "    return loss.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's test it\n",
    "memory = list(pool.prev_memory_states)\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)\n",
    "\n",
    "train_on_rollout(rollout_obs, rollout_actions,\n",
    "                 rollout_rewards, rollout_mask, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train \n",
    "\n",
    "just run train step and see if agent learns any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import trange\n",
    "from pandas import DataFrame\n",
    "moving_average = lambda x, **kw: DataFrame(\n",
    "    {'x': np.asarray(x)}).x.ewm(**kw).mean().values\n",
    "\n",
    "rewards_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in trange(15000):\n",
    "\n",
    "    memory = list(pool.prev_memory_states)\n",
    "    rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(\n",
    "        10)\n",
    "    train_on_rollout(rollout_obs, rollout_actions,\n",
    "                     rollout_rewards, rollout_mask, memory)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        rewards_history.append(np.mean(evaluate(agent, env, n_games=1)))\n",
    "        clear_output(True)\n",
    "        plt.plot(rewards_history, label='rewards')\n",
    "        plt.plot(moving_average(np.array(rewards_history),\n",
    "                                span=10), label='rewards ewma@10')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        if rewards_history[-1] >= 10000:\n",
    "            print(\"Your agent has just passed the minimum homework threshold\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relax and grab some refreshments while your agent is locked in an infinite loop of violence and death.\n",
    "\n",
    "__How to interpret plots:__\n",
    "\n",
    "The session reward is the easy thing: it should in general go up over time, but it's okay if it fluctuates ~~like crazy~~. It's also OK if it reward doesn't increase substantially before some 10k initial steps. However, if reward reaches zero and doesn't seem to get up over 2-3 evaluations, there's something wrong happening.\n",
    "\n",
    "\n",
    "Since we use a policy-based method, we also keep track of __policy entropy__ - the same one you used as a regularizer. The only important thing about it is that your entropy shouldn't drop too low (`< 0.1`) before your agent gets the yellow belt. Or at least it can drop there, but _it shouldn't stay there for long_.\n",
    "\n",
    "If it does, the culprit is likely:\n",
    "* Some bug in entropy computation. Remember that it is $ - \\sum p(a_i) \\cdot log p(a_i) $\n",
    "* Your agent architecture converges too fast. Increase entropy coefficient in actor loss. \n",
    "* Gradient explosion - just [clip gradients](https://stackoverflow.com/a/43486487) and maybe use a smaller network\n",
    "* Us. Or TF developers. Or aliens. Or lizardfolk. Contact us on forums before it's too late!\n",
    "\n",
    "If you're debugging, just run `logits, values = agent.step(batch_states)` and manually look into logits and values. This will reveal the problem 9 times out of 10: you'll likely see some NaNs or insanely large numbers or zeros. Try to catch the moment when this happens for the first time and investigate from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Final\" evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "final_rewards = evaluate(agent, env_monitor, n_games=20,)\n",
    "env_monitor.close()\n",
    "print(\"Final mean reward\", np.mean(final_rewards))\n",
    "\n",
    "video_names = list(filter(lambda s: s.endswith(\n",
    "    \".mp4\"), os.listdir(\"./kungfu_videos/\")))\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
