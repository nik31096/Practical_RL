{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from IPython.core import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "# If you are running on a server, launch xvfb to record game videos\n",
    "# Please make sure you have xvfb installed\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY = : 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kung-Fu, recurrent style\n",
    "\n",
    "In this notebook we'll once again train RL agent for for atari [KungFuMaster](https://gym.openai.com/envs/KungFuMaster-v0/), this time using recurrent neural networks.\n",
    "\n",
    "![http://www.retroland.com/wp-content/uploads/2011/07/King-Fu-Master.jpg](http://www.retroland.com/wp-content/uploads/2011/07/King-Fu-Master.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape: (1, 42, 42)\n",
      "Num actions: 14\n",
      "Action names: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik-96/anaconda3/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from atari_util import PreprocessAtari\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"KungFuMasterDeterministic-v0\")\n",
    "    env = PreprocessAtari(env, height=42, width=42,\n",
    "                          crop=lambda img: img[60:-30, 15:],\n",
    "                          color=False, n_frames=1)\n",
    "    return env\n",
    "\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"Observation shape:\", obs_shape)\n",
    "print(\"Num actions:\", n_actions)\n",
    "print(\"Action names:\", env.env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "for _ in range(100):\n",
    "    s, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.title('Game image')\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.show()\n",
    "\n",
    "plt.title('Agent observation')\n",
    "plt.imshow(s.reshape([42, 42]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POMDP setting\n",
    "\n",
    "The atari game we're working with is actually a POMDP: your agent needs to know timing at which enemies spawn and move, but cannot do so unless it has some memory. \n",
    "\n",
    "Let's design another agent that has a recurrent neural net memory to solve this. Here's a sketch.\n",
    "\n",
    "![img](img1.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# a special module that converts [batch, channel, w, h] to [batch, units]\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRecurrentAgent(nn.Module):\n",
    "    def __init__(self, obs_shape, n_actions, reuse=False):\n",
    "        \"\"\"A simple actor-critic agent\"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "\n",
    "        self.conv0 = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.conv1 = nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        self.hid = nn.Linear(512, 128)\n",
    "        #              input_size | hidden_size\n",
    "        self.rnn = nn.LSTMCell(128, 128)\n",
    "\n",
    "        self.logits = nn.Linear(128, n_actions)\n",
    "        self.state_value = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, prev_state, obs_t):\n",
    "        \"\"\"\n",
    "        Takes agent's previous step and observation, \n",
    "        returns next state and whatever it needs to learn (tf tensors)\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE: apply the whole neural net for one step here.\n",
    "        # See docs on self.rnn(...)\n",
    "        # the recurrent cell should take the last feedforward dense layer as input\n",
    "        obs_t = torch.tensor(obs_t, dtype=torch.float32)\n",
    "        x = self.conv0(obs_t)\n",
    "        x = nn.ELU()(x)\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ELU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ELU()(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.hid(x)\n",
    "        #                                                 \n",
    "        new_state = self.rnn(x, prev_state) # new_state = (h_1, c_1)\n",
    "        \n",
    "        logits = self.logits(new_state[0])\n",
    "        state_value = self.state_value(new_state[0])\n",
    "\n",
    "        return new_state, (logits, state_value)\n",
    "\n",
    "    def get_initial_state(self, batch_size):\n",
    "        \"\"\"Return a list of agent memory states at game start. Each state is a np array of shape [batch_size, ...]\"\"\"\n",
    "        return (Variable(torch.zeros((batch_size, 128))),\n",
    "                Variable(torch.zeros((batch_size, 128))))\n",
    "\n",
    "    def sample_actions(self, agent_outputs):\n",
    "        \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"\n",
    "        logits, state_values = agent_outputs\n",
    "        probs = F.softmax(logits)\n",
    "        return torch.multinomial(probs, 1)[:, 0].data.numpy()\n",
    "\n",
    "    def step(self, prev_state, obs_t):\n",
    "        \"\"\" like forward, but obs_t is not Variable \"\"\"\n",
    "        obs_t = Variable(torch.FloatTensor(np.array(obs_t)))\n",
    "        (h, c), (l, s) = self.forward(prev_state, obs_t)\n",
    "        return (h.detach(), c.detach()), (l.detach(), s.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parallel_games = 5\n",
    "gamma = 0.99\n",
    "\n",
    "agent = SimpleRecurrentAgent(obs_shape, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action logits:\n",
      " tensor([[-0.0245,  0.0722,  0.0690,  0.0588, -0.0955, -0.0425, -0.0540,  0.0859,\n",
      "         -0.0599, -0.0540, -0.0098, -0.0465,  0.0551,  0.0251]])\n",
      "state values:\n",
      " tensor([[0.0834]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik-96/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "state = [env.reset()]\n",
    "_, (logits, value) = agent.step(agent.get_initial_state(1), state)\n",
    "print(\"action logits:\\n\", logits)\n",
    "print(\"state values:\\n\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play!\n",
    "Let's build a function that measures agent's average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, n_games=1):\n",
    "    \"\"\"Plays an entire game start to end, returns session rewards.\"\"\"\n",
    "\n",
    "    game_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        # initial observation and memory\n",
    "        observation = env.reset()\n",
    "        prev_memories = agent.get_initial_state(1)\n",
    "\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            new_memories, readouts = agent.step(\n",
    "                prev_memories, observation[None, ...])\n",
    "            action = agent.sample_actions(readouts)\n",
    "\n",
    "            observation, reward, done, info = env.step(action[0])\n",
    "\n",
    "            total_reward += reward\n",
    "            prev_memories = new_memories\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        game_rewards.append(total_reward)\n",
    "    return game_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik-96/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/nik-96/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/nik-96/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/nik-96/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.0, 400.0, 200.0]\n"
     ]
    }
   ],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "rw = evaluate(agent, env_monitor, n_games=3,)\n",
    "env_monitor.close()\n",
    "print(rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./kungfu_videos/openaigym.video.0.24631.video000000.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s: s.endswith(\n",
    "    \".mp4\"), os.listdir(\"./kungfu_videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on parallel games\n",
    "\n",
    "We introduce a class called EnvPool - it's a tool that handles multiple environments for you. Here's how it works:\n",
    "![img](img2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik-96/anaconda3/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "from env_pool import EnvPool\n",
    "pool = EnvPool(agent, make_env, n_parallel_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gonna train our agent on a thing called __rollouts:__\n",
    "![img](img3.jpg)\n",
    "\n",
    "A rollout is just a sequence of T observations, actions and rewards that agent took consequently.\n",
    "* First __s0__ is not necessarily initial state for the environment\n",
    "* Final state is not necessarily terminal\n",
    "* We sample several parallel rollouts for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik-96/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/nik-96/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# for each of n_parallel_games, take 10 steps\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions shape: (5, 10)\n",
      "Rewards shape: (5, 10)\n",
      "Mask shape: (5, 10)\n",
      "Observations shape:  (5, 10, 1, 42, 42)\n"
     ]
    }
   ],
   "source": [
    "print(\"Actions shape:\", rollout_actions.shape)\n",
    "print(\"Rewards shape:\", rollout_rewards.shape)\n",
    "print(\"Mask shape:\", rollout_mask.shape)\n",
    "print(\"Observations shape: \", rollout_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic objective\n",
    "\n",
    "Here we define a loss function that uses rollout above to train advantage actor-critic agent.\n",
    "\n",
    "\n",
    "Our loss consists of three components:\n",
    "\n",
    "* __The policy \"loss\"__\n",
    " $$ \\hat J = {1 \\over T} \\cdot \\sum_t { \\log \\pi(a_t | s_t) } \\cdot A_{const}(s,a) $$\n",
    "  * This function has no meaning in and of itself, but it was built such that\n",
    "  * $ \\nabla \\hat J = {1 \\over T} \\cdot \\sum_t { \\nabla \\log \\pi(a_t | s_t) } \\cdot A(s,a) \\approx \\nabla E_{s, a \\sim \\pi} R(s,a) $\n",
    "  * Therefore if we __maximize__ J_hat with gradient descent we will maximize expected reward\n",
    "  \n",
    "  \n",
    "* __The value \"loss\"__\n",
    "  $$ L_{td} = {1 \\over T} \\cdot \\sum_t { [r + \\gamma \\cdot V_{const}(s_{t+1}) - V(s_t)] ^ 2 }$$\n",
    "  * Ye Olde TD_loss from q-learning and alike\n",
    "  * If we minimize this loss, V(s) will converge to $V_\\pi(s) = E_{a \\sim \\pi(a | s)} R(s,a) $\n",
    "\n",
    "\n",
    "* __Entropy Regularizer__\n",
    "  $$ H = - {1 \\over T} \\sum_t \\sum_a {\\pi(a|s_t) \\cdot \\log \\pi (a|s_t)}$$\n",
    "  * If we __maximize__ entropy we discourage agent from predicting zero probability to actions\n",
    "  prematurely (a.k.a. exploration)\n",
    "  \n",
    "  \n",
    "So we optimize a linear combination of $L_{td}$ $- \\hat J$, $-H$\n",
    "  \n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "__One more thing:__ since we train on T-step rollouts, we can use N-step formula for advantage for free:\n",
    "  * At the last step, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot V(s_{t+1}) - V(s) $\n",
    "  * One step earlier, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot r(s_{t+1}, a_{t+1}) + \\gamma ^ 2 \\cdot V(s_{t+2}) - V(s) $\n",
    "  * Et cetera, et cetera. This way agent starts training much faster since it's estimate of A(s,a) depends less on his (imperfect) value function and more on actual rewards. There's also a [nice generalization](https://arxiv.org/abs/1506.02438) of this.\n",
    "\n",
    "\n",
    "__Note:__ it's also a good idea to scale rollout_len up to learn longer sequences. You may wish set it to >=20 or to start at 10 and then scale up as time passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take an integer vector (tensor of variable) and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y.data if isinstance(y, Variable) else y\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(\n",
    "        y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    return Variable(y_one_hot) if isinstance(y, Variable) else y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "def train_on_rollout(states, actions, rewards, is_not_done, prev_memory_states, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # cast everything into a variable\n",
    "    # shape: [batch_size, time, c, h, w]\n",
    "    states = Variable(torch.FloatTensor(np.array(states)))\n",
    "    actions = Variable(torch.IntTensor(np.array(actions))\n",
    "                       )   # shape: [batch_size, time]\n",
    "    rewards = Variable(torch.FloatTensor(np.array(rewards))\n",
    "                       )  # shape: [batch_size, time]\n",
    "    is_not_done = Variable(torch.FloatTensor(\n",
    "        is_not_done.astype('float32')))  # shape: [batch_size, time]\n",
    "    rollout_length = rewards.shape[1] - 1\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    memory = [m.detach() for m in prev_memory_states]\n",
    "\n",
    "    logits = []  # append logit sequence here\n",
    "    state_values = []  # append state values here\n",
    "    for t in range(rewards.shape[1]):\n",
    "        obs_t = states[:, t]\n",
    "\n",
    "        # use agent to comute logits_t and state values_t.\n",
    "        # append them to logits and state_values array\n",
    "\n",
    "        memory, (logits_t, values_t) = <YOUR CODE >\n",
    "\n",
    "        logits.append(logits_t)\n",
    "        state_values.append(values_t)\n",
    "\n",
    "    logits = torch.stack(logits, dim=1)\n",
    "    state_values = torch.stack(state_values, dim=1)\n",
    "    probas = F.softmax(logits, dim=2)\n",
    "    logprobas = F.log_softmax(logits, dim=2)\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    actions_one_hot = to_one_hot(actions, n_actions).view(\n",
    "        actions.shape[0], actions.shape[1], n_actions)\n",
    "    logprobas_for_actions = torch.sum(logprobas * actions_one_hot, dim=-1)\n",
    "\n",
    "    # Now let's compute two loss components:\n",
    "    # 1) Policy gradient objective.\n",
    "    # Notes: Please don't forget to call .detach() on advantage term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    J_hat = 0  # policy objective as in the formula for J_hat\n",
    "\n",
    "    # 2) Temporal difference MSE for state values\n",
    "    # Notes: Please don't forget to call on V(s') term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    value_loss = 0\n",
    "\n",
    "    cumulative_returns = state_values[:, -1].detach()\n",
    "\n",
    "    for t in reversed(range(rollout_length)):\n",
    "        r_t = rewards[:, t]                                # current rewards\n",
    "        # current state values\n",
    "        V_t = state_values[:, t]\n",
    "        V_next = state_values[:, t + 1].detach()           # next state values\n",
    "        # log-probability of a_t in s_t\n",
    "        logpi_a_s_t = logprobas_for_actions[:, t]\n",
    "\n",
    "        # update G_t = r_t + gamma * G_{t+1} as we did in week6 reinforce\n",
    "        cumulative_returns = G_t = r_t + gamma * cumulative_returns\n",
    "\n",
    "        # Compute temporal difference error (MSE for V(s))\n",
    "        value_loss += <YOUR CODE >\n",
    "\n",
    "        # compute advantage A(s_t, a_t) using cumulative returns and V(s_t) as baseline\n",
    "        advantage = <YOUR CODE >\n",
    "        advantage = advantage.detach()\n",
    "\n",
    "        # compute policy pseudo-loss aka -J_hat.\n",
    "        J_hat += <YOUR CODE >\n",
    "\n",
    "    # regularize with entropy\n",
    "    entropy_reg = <compute entropy regularizer >\n",
    "\n",
    "    # add-up three loss components and average over time\n",
    "    loss = -J_hat / rollout_length +\\\n",
    "        value_loss / rollout_length +\\\n",
    "           -0.01 * entropy_reg\n",
    "\n",
    "    # Gradient descent step\n",
    "    < your code >\n",
    "\n",
    "    return loss.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's test it\n",
    "memory = list(pool.prev_memory_states)\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)\n",
    "\n",
    "train_on_rollout(rollout_obs, rollout_actions,\n",
    "                 rollout_rewards, rollout_mask, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train \n",
    "\n",
    "just run train step and see if agent learns any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import trange\n",
    "from pandas import DataFrame\n",
    "moving_average = lambda x, **kw: DataFrame(\n",
    "    {'x': np.asarray(x)}).x.ewm(**kw).mean().values\n",
    "\n",
    "rewards_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in trange(15000):\n",
    "\n",
    "    memory = list(pool.prev_memory_states)\n",
    "    rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(\n",
    "        10)\n",
    "    train_on_rollout(rollout_obs, rollout_actions,\n",
    "                     rollout_rewards, rollout_mask, memory)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        rewards_history.append(np.mean(evaluate(agent, env, n_games=1)))\n",
    "        clear_output(True)\n",
    "        plt.plot(rewards_history, label='rewards')\n",
    "        plt.plot(moving_average(np.array(rewards_history),\n",
    "                                span=10), label='rewards ewma@10')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        if rewards_history[-1] >= 10000:\n",
    "            print(\"Your agent has just passed the minimum homework threshold\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relax and grab some refreshments while your agent is locked in an infinite loop of violence and death.\n",
    "\n",
    "__How to interpret plots:__\n",
    "\n",
    "The session reward is the easy thing: it should in general go up over time, but it's okay if it fluctuates ~~like crazy~~. It's also OK if it reward doesn't increase substantially before some 10k initial steps. However, if reward reaches zero and doesn't seem to get up over 2-3 evaluations, there's something wrong happening.\n",
    "\n",
    "\n",
    "Since we use a policy-based method, we also keep track of __policy entropy__ - the same one you used as a regularizer. The only important thing about it is that your entropy shouldn't drop too low (`< 0.1`) before your agent gets the yellow belt. Or at least it can drop there, but _it shouldn't stay there for long_.\n",
    "\n",
    "If it does, the culprit is likely:\n",
    "* Some bug in entropy computation. Remember that it is $ - \\sum p(a_i) \\cdot log p(a_i) $\n",
    "* Your agent architecture converges too fast. Increase entropy coefficient in actor loss. \n",
    "* Gradient explosion - just [clip gradients](https://stackoverflow.com/a/43486487) and maybe use a smaller network\n",
    "* Us. Or TF developers. Or aliens. Or lizardfolk. Contact us on forums before it's too late!\n",
    "\n",
    "If you're debugging, just run `logits, values = agent.step(batch_states)` and manually look into logits and values. This will reveal the problem 9 times out of 10: you'll likely see some NaNs or insanely large numbers or zeros. Try to catch the moment when this happens for the first time and investigate from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Final\" evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "final_rewards = evaluate(agent, env_monitor, n_games=20,)\n",
    "env_monitor.close()\n",
    "print(\"Final mean reward\", np.mean(final_rewards))\n",
    "\n",
    "video_names = list(filter(lambda s: s.endswith(\n",
    "    \".mp4\"), os.listdir(\"./kungfu_videos/\")))\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
