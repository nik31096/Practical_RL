{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from IPython.core import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "# If you are running on a server, launch xvfb to record game videos\n",
    "# Please make sure you have xvfb installed\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY = : 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kung-Fu, recurrent style\n",
    "\n",
    "In this notebook we'll once again train RL agent for for atari [KungFuMaster](https://gym.openai.com/envs/KungFuMaster-v0/), this time using recurrent neural networks.\n",
    "\n",
    "![http://www.retroland.com/wp-content/uploads/2011/07/King-Fu-Master.jpg](http://www.retroland.com/wp-content/uploads/2011/07/King-Fu-Master.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape: (1, 42, 42)\n",
      "Num actions: 14\n",
      "Action names: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from atari_util import PreprocessAtari\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"KungFuMasterDeterministic-v0\")\n",
    "    env = PreprocessAtari(env, height=42, width=42,\n",
    "                          crop=lambda img: img[60:-30, 15:],\n",
    "                          color=False, n_frames=1)\n",
    "    return env\n",
    "\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"Observation shape:\", obs_shape)\n",
    "print(\"Num actions:\", n_actions)\n",
    "print(\"Action names:\", env.env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "for _ in range(100):\n",
    "    s, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.title('Game image')\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.show()\n",
    "\n",
    "plt.title('Agent observation')\n",
    "plt.imshow(s.reshape([42, 42]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POMDP setting\n",
    "\n",
    "The atari game we're working with is actually a POMDP: your agent needs to know timing at which enemies spawn and move, but cannot do so unless it has some memory. \n",
    "\n",
    "Let's design another agent that has a recurrent neural net memory to solve this. Here's a sketch.\n",
    "\n",
    "![img](img1.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# a special module that converts [batch, channel, w, h] to [batch, units]\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRecurrentAgent(nn.Module):\n",
    "    def __init__(self, obs_shape, n_actions, reuse=False):\n",
    "        \"\"\"A simple actor-critic agent\"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "\n",
    "        self.conv0 = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.conv1 = nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        self.hid = nn.Linear(512, 128)\n",
    "        #              input_size | hidden_size\n",
    "        self.rnn = nn.LSTMCell(128, 128)\n",
    "\n",
    "        self.logits = nn.Linear(128, n_actions)\n",
    "        self.state_value = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, prev_state, obs_t):\n",
    "        \"\"\"\n",
    "        Takes agent's previous step and observation, \n",
    "        returns next state and whatever it needs to learn (tf tensors)\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE: apply the whole neural net for one step here.\n",
    "        # See docs on self.rnn(...)\n",
    "        # the recurrent cell should take the last feedforward dense layer as input\n",
    "        obs_t = torch.tensor(obs_t, dtype=torch.float32)\n",
    "        x = self.conv0(obs_t)\n",
    "        x = nn.ELU()(x)\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ELU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ELU()(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.hid(x)\n",
    "        #                                                 \n",
    "        new_state = self.rnn(x, prev_state) # new_state = (h_1, c_1)\n",
    "        \n",
    "        logits = self.logits(new_state[0])\n",
    "        state_value = self.state_value(new_state[0])\n",
    "\n",
    "        return new_state, (logits, state_value)\n",
    "\n",
    "    def get_initial_state(self, batch_size):\n",
    "        \"\"\"Return a list of agent memory states at game start. Each state is a np array of shape [batch_size, ...]\"\"\"\n",
    "        return (Variable(torch.zeros((batch_size, 128))),\n",
    "                Variable(torch.zeros((batch_size, 128))))\n",
    "\n",
    "    def sample_actions(self, agent_outputs):\n",
    "        \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"\n",
    "        logits, state_values = agent_outputs\n",
    "        probs = F.softmax(logits)\n",
    "        return torch.multinomial(probs, 1)[:, 0].data.numpy()\n",
    "\n",
    "    def step(self, prev_state, obs_t):\n",
    "        \"\"\" like forward, but obs_t is not Variable \"\"\"\n",
    "        obs_t = Variable(torch.FloatTensor(np.array(obs_t)))\n",
    "        (h, c), (l, s) = self.forward(prev_state, obs_t)\n",
    "        return (h.detach(), c.detach()), (l.detach(), s.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parallel_games = 5\n",
    "gamma = 0.99\n",
    "\n",
    "agent = SimpleRecurrentAgent(obs_shape, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action logits:\n",
      " tensor([[-0.0665,  0.0751, -0.0910, -0.0582,  0.0683,  0.0322,  0.0203, -0.0136,\n",
      "         -0.0062, -0.0717, -0.0149,  0.0195,  0.0678, -0.0504]])\n",
      "state values:\n",
      " tensor([[0.0437]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "state = [env.reset()]\n",
    "_, (logits, value) = agent.step(agent.get_initial_state(1), state)\n",
    "print(\"action logits:\\n\", logits)\n",
    "print(\"state values:\\n\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play!\n",
    "Let's build a function that measures agent's average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, n_games=1):\n",
    "    \"\"\"Plays an entire game start to end, returns session rewards.\"\"\"\n",
    "\n",
    "    game_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        # initial observation and memory\n",
    "        observation = env.reset()\n",
    "        prev_memories = agent.get_initial_state(1)\n",
    "\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            new_memories, readouts = agent.step(\n",
    "                prev_memories, observation[None, ...])\n",
    "            action = agent.sample_actions(readouts)\n",
    "\n",
    "            observation, reward, done, info = env.step(action[0])\n",
    "\n",
    "            total_reward += reward\n",
    "            prev_memories = new_memories\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        game_rewards.append(total_reward)\n",
    "    return game_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/nik/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/nik/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/nik/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000.0, 0.0, 1100.0]\n"
     ]
    }
   ],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "rw = evaluate(agent, env_monitor, n_games=3,)\n",
    "env_monitor.close()\n",
    "print(rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./kungfu_videos/openaigym.video.0.29737.video000000.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s: s.endswith(\n",
    "    \".mp4\"), os.listdir(\"./kungfu_videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on parallel games\n",
    "\n",
    "We introduce a class called EnvPool - it's a tool that handles multiple environments for you. Here's how it works:\n",
    "![img](img2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_parallel_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_pool import EnvPool\n",
    "pool = EnvPool(agent, make_env, n_parallel_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gonna train our agent on a thing called __rollouts:__\n",
    "![img](img3.jpg)\n",
    "\n",
    "A rollout is just a sequence of T observations, actions and rewards that agent took consequently.\n",
    "* First __s0__ is not necessarily initial state for the environment\n",
    "* Final state is not necessarily terminal\n",
    "* We sample several parallel rollouts for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/nik/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# for each of n_parallel_games, take 10 steps\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions shape: (5, 10)\n",
      "Rewards shape: (5, 10)\n",
      "Mask shape: (5, 10)\n",
      "Observations shape:  (5, 10, 1, 42, 42)\n"
     ]
    }
   ],
   "source": [
    "print(\"Actions shape:\", rollout_actions.shape)\n",
    "print(\"Rewards shape:\", rollout_rewards.shape)\n",
    "print(\"Mask shape:\", rollout_mask.shape)\n",
    "print(\"Observations shape: \", rollout_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic objective\n",
    "\n",
    "Here we define a loss function that uses rollout above to train advantage actor-critic agent.\n",
    "\n",
    "\n",
    "Our loss consists of three components:\n",
    "\n",
    "* __The policy \"loss\"__\n",
    " $$ \\hat J = {1 \\over T} \\cdot \\sum_t { \\log \\pi(a_t | s_t) } \\cdot A_{const}(s,a) $$\n",
    "  * This function has no meaning in and of itself, but it was built such that\n",
    "  * $ \\nabla \\hat J = {1 \\over T} \\cdot \\sum_t { \\nabla \\log \\pi(a_t | s_t) } \\cdot A(s,a) \\approx \\nabla E_{s, a \\sim \\pi} R(s,a) $\n",
    "  * Therefore if we __maximize__ J_hat with gradient descent we will maximize expected reward\n",
    "  \n",
    "  \n",
    "* __The value \"loss\"__\n",
    "  $$ L_{td} = {1 \\over T} \\cdot \\sum_t { [r + \\gamma \\cdot V_{const}(s_{t+1}) - V(s_t)] ^ 2 }$$\n",
    "  * Ye Olde TD_loss from q-learning and alike\n",
    "  * If we minimize this loss, V(s) will converge to $V_\\pi(s) = E_{a \\sim \\pi(a | s)} R(s,a) $\n",
    "\n",
    "\n",
    "* __Entropy Regularizer__\n",
    "  $$ H = - {1 \\over T} \\sum_t \\sum_a {\\pi(a|s_t) \\cdot \\log \\pi (a|s_t)}$$\n",
    "  * If we __maximize__ entropy we discourage agent from predicting zero probability to actions\n",
    "  prematurely (a.k.a. exploration)\n",
    "  \n",
    "  \n",
    "So we optimize a linear combination of $L_{td}$ $- \\hat J$, $-H$\n",
    "  \n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "__One more thing:__ since we train on T-step rollouts, we can use N-step formula for advantage for free:\n",
    "  * At the last step, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot V(s_{t+1}) - V(s) $\n",
    "  * One step earlier, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot r(s_{t+1}, a_{t+1}) + \\gamma ^ 2 \\cdot V(s_{t+2}) - V(s) $\n",
    "  * Et cetera, et cetera. This way agent starts training much faster since it's estimate of A(s,a) depends less on his (imperfect) value function and more on actual rewards. There's also a [nice generalization](https://arxiv.org/abs/1506.02438) of this.\n",
    "\n",
    "\n",
    "__Note:__ it's also a good idea to scale rollout_len up to learn longer sequences. You may wish set it to >=20 or to start at 10 and then scale up as time passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take an integer vector (tensor of variable) and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y.data if isinstance(y, Variable) else y\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(\n",
    "        y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    return Variable(y_one_hot) if isinstance(y, Variable) else y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_rollout(states, actions, rewards, is_not_done, prev_memory_states, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # cast everything into a variable\n",
    "    # shape: [batch_size, time, c, h, w]\n",
    "    states = Variable(torch.FloatTensor(np.array(states)))\n",
    "    actions = Variable(torch.IntTensor(np.array(actions)))   # shape: [batch_size, time]\n",
    "    rewards = Variable(torch.FloatTensor(np.array(rewards)))  # shape: [batch_size, time]\n",
    "    is_not_done = Variable(torch.FloatTensor(is_not_done.astype('float32')))  # shape: [batch_size, time]\n",
    "    rollout_length = rewards.shape[1] - 1\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    memory = [m.detach() for m in prev_memory_states]\n",
    "\n",
    "    logits = []  # append logit sequence here\n",
    "    state_values = []  # append state values here\n",
    "    for t in range(rewards.shape[1]):\n",
    "        obs_t = states[:, t]\n",
    "\n",
    "        # use agent to comute logits_t and state values_t.\n",
    "        # append them to logits and state_values array\n",
    "\n",
    "        memory, (logits_t, values_t) = agent(memory, obs_t)\n",
    "        logits.append(logits_t)\n",
    "        state_values.append(values_t)\n",
    "    \n",
    "\n",
    "    logits = torch.stack(logits, dim=1)\n",
    "    state_values = torch.stack(state_values, dim=1)\n",
    "    probas = F.softmax(logits, dim=2)\n",
    "    logprobas = F.log_softmax(logits, dim=2)\n",
    "    print(\"#### Shapes ####\")\n",
    "    print(\"logits shape: {}, state_values shape: {}, probas shape: {}, logprobas shape: {}\".format(\n",
    "            logits.shape, state_values.shape, probas.shape, logprobas.shape))\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    actions_one_hot = to_one_hot(actions, n_actions).view(\n",
    "        actions.shape[0], actions.shape[1], n_actions)\n",
    "    logprobas_for_actions = torch.sum(logprobas * actions_one_hot, dim=-1)\n",
    "    print(\"logprobs for actions shape\", logprobas_for_actions.shape)\n",
    "\n",
    "    # Now let's compute two loss components:\n",
    "    # 1) Policy gradient objective.\n",
    "    # Notes: Please don't forget to call .detach() on advantage term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    # policy objective as in the formula for J_hat\n",
    "    # torch.mean(logprobas_for_actions*)\n",
    "    advantage = torch.FloatTensor([])\n",
    "    J_hat = 0 \n",
    "\n",
    "    # 2) Temporal difference MSE for state values\n",
    "    # Notes: Please don't forget to call on V(s') term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    value_loss = 0\n",
    "\n",
    "    cumulative_returns = state_values[:, -1].detach()\n",
    "\n",
    "    for t in reversed(range(rollout_length)):\n",
    "        r_t = rewards[:, t]                                # current rewards\n",
    "        # current state values\n",
    "        V_t = state_values[:, t]\n",
    "        V_next = state_values[:, t + 1].detach()           # next state values\n",
    "        # log-probability of a_t in s_t\n",
    "        logpi_a_s_t = logprobas_for_actions[:, t]\n",
    "\n",
    "        # update G_t = r_t + gamma * G_{t+1} as we did in week6 reinforce\n",
    "        cumulative_returns = G_t = r_t + gamma * cumulative_returns\n",
    "\n",
    "        # Compute temporal difference error (MSE for V(s))\n",
    "        value_loss += torch.mean((cumulative_returns - V_t)**2)\n",
    "\n",
    "        # compute advantage A(s_t, a_t) using cumulative returns and V(s_t) as baseline\n",
    "        advantage = cumulative_returns - V_t\n",
    "        advantage = advantage.detach()\n",
    "\n",
    "        # compute policy pseudo-loss aka -J_hat.\n",
    "        J_hat += torch.mean(logpi_a_s_t*advantage)\n",
    "\n",
    "    # regularize with entropy\n",
    "    entropy_reg = torch.mean(logpi_a_s_t)\n",
    "\n",
    "    # add-up three loss components and average over time\n",
    "    loss = -J_hat / rollout_length + value_loss / rollout_length + -0.01 * entropy_reg\n",
    "\n",
    "    # Gradient descent step\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    return loss.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/nik/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Shapes ####\n",
      "logits shape: torch.Size([5, 10, 14]), state_values shape: torch.Size([5, 10, 1]), probas shape: torch.Size([5, 10, 14]), logprobas shape: torch.Size([5, 10, 14])\n",
      "logprobs for actions shape torch.Size([5, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(0.02153163, dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's test it\n",
    "memory = list(pool.prev_memory_states)\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)\n",
    "\n",
    "train_on_rollout(rollout_obs, rollout_actions,\n",
    "                 rollout_rewards, rollout_mask, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train \n",
    "\n",
    "just run train step and see if agent learns any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import trange\n",
    "from pandas import DataFrame\n",
    "moving_average = lambda x, **kw: DataFrame(\n",
    "    {'x': np.asarray(x)}).x.ewm(**kw).mean().values\n",
    "\n",
    "rewards_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in trange(15000):\n",
    "\n",
    "    memory = list(pool.prev_memory_states)\n",
    "    rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(\n",
    "        10)\n",
    "    train_on_rollout(rollout_obs, rollout_actions,\n",
    "                     rollout_rewards, rollout_mask, memory)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        rewards_history.append(np.mean(evaluate(agent, env, n_games=1)))\n",
    "        clear_output(True)\n",
    "        plt.plot(rewards_history, label='rewards')\n",
    "        plt.plot(moving_average(np.array(rewards_history),\n",
    "                                span=10), label='rewards ewma@10')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        if rewards_history[-1] >= 10000:\n",
    "            print(\"Your agent has just passed the minimum homework threshold\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relax and grab some refreshments while your agent is locked in an infinite loop of violence and death.\n",
    "\n",
    "__How to interpret plots:__\n",
    "\n",
    "The session reward is the easy thing: it should in general go up over time, but it's okay if it fluctuates ~~like crazy~~. It's also OK if it reward doesn't increase substantially before some 10k initial steps. However, if reward reaches zero and doesn't seem to get up over 2-3 evaluations, there's something wrong happening.\n",
    "\n",
    "\n",
    "Since we use a policy-based method, we also keep track of __policy entropy__ - the same one you used as a regularizer. The only important thing about it is that your entropy shouldn't drop too low (`< 0.1`) before your agent gets the yellow belt. Or at least it can drop there, but _it shouldn't stay there for long_.\n",
    "\n",
    "If it does, the culprit is likely:\n",
    "* Some bug in entropy computation. Remember that it is $ - \\sum p(a_i) \\cdot log p(a_i) $\n",
    "* Your agent architecture converges too fast. Increase entropy coefficient in actor loss. \n",
    "* Gradient explosion - just [clip gradients](https://stackoverflow.com/a/43486487) and maybe use a smaller network\n",
    "* Us. Or TF developers. Or aliens. Or lizardfolk. Contact us on forums before it's too late!\n",
    "\n",
    "If you're debugging, just run `logits, values = agent.step(batch_states)` and manually look into logits and values. This will reveal the problem 9 times out of 10: you'll likely see some NaNs or insanely large numbers or zeros. Try to catch the moment when this happens for the first time and investigate from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Final\" evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "final_rewards = evaluate(agent, env_monitor, n_games=20,)\n",
    "env_monitor.close()\n",
    "print(\"Final mean reward\", np.mean(final_rewards))\n",
    "\n",
    "video_names = list(filter(lambda s: s.endswith(\n",
    "    \".mp4\"), os.listdir(\"./kungfu_videos/\")))\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
